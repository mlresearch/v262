---
title: Scaling laws for post-training quantized large language models
section: Model Efficiency \& Compression
abstract: 'Generalization abilities of well-trained large language models (LLMs) are
  known to scale predictably as a function of model size. In contrast to the existence
  of practical scaling laws governing pre-training, the quality of LLMs after post-training
  compression remains highly unpredictable, often requiring case-by-case validation
  in practice. In this work, we attempted to close this gap for post-training weight
  quantization of LLMs by conducting a systematic empirical study on multiple LLM
  families quantized to numerous low-precision tensor data types using popular weight
  quantization techniques. We identified key scaling factors pertaining to characteristics
  of the local loss landscape, based on which the performance of quantized LLMs can
  be reasonably well predicted by a statistical model. '
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: xu24a
month: 0
tex_title: Scaling laws for post-training quantized large language models
firstpage: 270
lastpage: 285
page: 270-285
order: 270
cycles: false
bibtex_author: Xu, Zifei and Y Lan, Alexander and Yazar, Wanzin and Webb, Tristan
  and Sharify, Sayeh and Wang, Xin
author:
- given: Zifei
  family: Xu
- given: Alexander
  family: Y Lan
- given: Wanzin
  family: Yazar
- given: Tristan
  family: Webb
- given: Sayeh
  family: Sharify
- given: Xin
  family: Wang
date: 2024-12-10
address:
container-title: Proceedings of The 4th NeurIPS Efficient Natural Language and Speech
  Processing Workshop
volume: '262'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 12
  - 10
pdf: https://raw.githubusercontent.com/mlresearch/v262/main/assets/xu24a/xu24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
