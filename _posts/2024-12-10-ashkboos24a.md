---
title: Computational Bottlenecks of Training Small-scale Large Language Models
section: Training
abstract: While large language models (LLMs) dominate the AI landscape, Small-scale
  large Language Models (SLMs) are gaining attention due to cost and efficiency demands
  from consumers. However, there is limited research on the training behavior and
  computational requirements of SLMs. In this study, we explore the computational
  bottlenecks of training SLMs (up to 2B parameters) by examining the effects of various
  hyperparameters and configurations, including GPU type, batch size, model size,
  communication protocol, attention type, and the number of GPUs. We assess these
  factors on popular cloud services using metrics such as loss per dollar and tokens
  per second. Our findings aim to support the broader adoption and optimization of
  language model training for low-resource AI research institutes.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: ashkboos24a
month: 0
tex_title: Computational Bottlenecks of Training Small-scale Large Language Models
firstpage: 14
lastpage: 21
page: 14-21
order: 14
cycles: false
bibtex_author: Ashkboos, Saleh and Iman Mirzadeh, Seyed and Alizadeh-Vahid, Keivan
  and Hossein Sekhavat, Mohammad and Nabi, Moin and Farajtabar, Mehrdad and Faghri,
  Fartash
author:
- given: Saleh
  family: Ashkboos
- given: Seyed
  family: Iman Mirzadeh
- given: Keivan
  family: Alizadeh-Vahid
- given: Mohammad
  family: Hossein Sekhavat
- given: Moin
  family: Nabi
- given: Mehrdad
  family: Farajtabar
- given: Fartash
  family: Faghri
date: 2024-12-10
address:
container-title: Proceedings of The 4th NeurIPS Efficient Natural Language and Speech
  Processing Workshop
volume: '262'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 12
  - 10
pdf: https://raw.githubusercontent.com/mlresearch/v262/main/assets/ashkboos24a/ashkboos24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
