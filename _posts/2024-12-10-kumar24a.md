---
title: Residual vector quantization for KV cache compression in large language model
section: Inference
abstract: 'KV cache compression methods have mainly relied on scalar quantization
  techniques to reduce the memory requirements during decoding. In this work, we apply
  residual vector quantization, which has been widely used for high fidelity audio
  compression, to compress KV cache in large language models (LLM). We adapt the standard
  recipe with minimal changes to compress the output of any key or value projection
  matrix in a pretrained LLM: we scale the vector by its standard deviation, divide
  channels into groups and then quantize each group with the same residual vector
  quantizer. We learn the codebook using exponential moving average and there are
  no other learnable parameters including the input and output projections normally
  used in a vector quantization set up. We find that a residual depth of 8 recovers
  most of the performance of the unquantized model. We also find that grouping non-contiguous
  channels together works better than grouping contiguous channels for compressing
  key matrix and the method further benefits from a light weight finetuning of LLM
  together with the quantization. Overall, the proposed technique is competitive with
  existing quantization methods while being much simpler and results in Â 5.5x compression
  compared to half precision.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: kumar24a
month: 0
tex_title: Residual vector quantization for {KV} cache compression in large language
  model
firstpage: 485
lastpage: 490
page: 485-490
order: 485
cycles: false
bibtex_author: Kumar, Ankur
author:
- given: Ankur
  family: Kumar
date: 2024-12-10
address:
container-title: Proceedings of The 4th NeurIPS Efficient Natural Language and Speech
  Processing Workshop
volume: '262'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 12
  - 10
pdf: https://raw.githubusercontent.com/mlresearch/v262/main/assets/kumar24a/kumar24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
