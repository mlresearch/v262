---
title: 'VL-Mamba: Exploring State Space Models for Multimodal Learning'
section: Model Design \& Architecture
abstract: Multimodal large language models (MLLMs) have gained considerable attention
  due to their ability to integrate visual and textual information, enhancing understanding
  and providing context for complex tasks. While Transformer-based architectures have
  been the dominant framework for MLLMs, recent studies suggest that state space models
  (SSMs) like Mamba can achieve competitive or even superior performance. However,
  no prior research has investigated the potential of SSMs to replace Transformers
  in multimodal tasks, which are inherently more challenging due to the heterogeneity
  of visual and language data and the complexities of aligning these modalities. In
  this paper, we introduce VL-Mamba, the first study to explore the application of
  state space models in multimodal learning tasks. VL-Mamba leverages a pretrained
  Mamba language model as its core, and we propose a novel MultiModal Connector (MMC)
  that incorporates a Vision Selective Scan (VSS) module to improve visual sequence
  modeling. We empirically explore how to effectively apply the 2D vision selective
  scan mechanism for multimodal learning and the combinations of different vision
  encoders and variants of pretrained Mamba language models. Our experiments across
  multiple multimodal benchmarks demonstrate that VL-Mamba achieves competitive performance
  against small MLLMs of similar size, and in some cases, surpasses larger models
  such as the 7B and 13B versions of LLaVA-1.5. These results suggest that state space
  models have the potential to serve as an alternative to Transformers in multimodal
  learning tasks.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: qiao24a
month: 0
tex_title: "{VL-Mamba}: Exploring State Space Models for Multimodal Learning"
firstpage: 102
lastpage: 113
page: 102-113
order: 102
cycles: false
bibtex_author: Qiao, Yanyuan and Yu, Zheng and Zhao, Zijia and Chen, Sihan and Sun,
  Mingzhen and Guo, Longteng and Wu, Qi and Liu, Jing
author:
- given: Yanyuan
  family: Qiao
- given: Zheng
  family: Yu
- given: Zijia
  family: Zhao
- given: Sihan
  family: Chen
- given: Mingzhen
  family: Sun
- given: Longteng
  family: Guo
- given: Qi
  family: Wu
- given: Jing
  family: Liu
date: 2024-12-10
address:
container-title: Proceedings of The 4th NeurIPS Efficient Natural Language and Speech
  Processing Workshop
volume: '262'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 12
  - 10
pdf: https://raw.githubusercontent.com/mlresearch/v262/main/assets/qiao24a/qiao24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
