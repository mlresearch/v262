---
title: Inference-Friendly Models With MixAttention
section: Inference
abstract: The size of the key-value (KV) cache plays a critical role in determining
  both the maximum context length and the number of concurrent requests supported
  during inference in modern language models. The KV cache size grows proportionally
  with the number of attention heads and the tokens processed, leading to increased
  memory consumption and slower inference for long inputs. In this work, we explore
  the use of MixAttention, a model architecture modification closely related to a
  blog published by Character.AI. MixAttention combines sliding window attention,
  where only a small subset of recent tokens is stored in the KV cache, with KV cache
  sharing across layers. Our experiments demonstrate that MixAttention significantly
  reduces memory usage and improves inference speed without sacrificing model performance
  in both short and long-context tasks. We also explore various configurations of
  this architecture, identifying those that maintain quality across evaluation metrics
  while optimizing resource efficiency.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: rajput24a
month: 0
tex_title: Inference-Friendly Models With {MixAttention}
firstpage: 370
lastpage: 381
page: 370-381
order: 370
cycles: false
bibtex_author: Rajput, Shashank and Sheng, Ying and Owen, Sean and Chiley, Vitaliy
author:
- given: Shashank
  family: Rajput
- given: Ying
  family: Sheng
- given: Sean
  family: Owen
- given: Vitaliy
  family: Chiley
date: 2024-12-10
address:
container-title: Proceedings of The 4th NeurIPS Efficient Natural Language and Speech
  Processing Workshop
volume: '262'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 12
  - 10
pdf: https://raw.githubusercontent.com/mlresearch/v262/main/assets/rajput24a/rajput24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
