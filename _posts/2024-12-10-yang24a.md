---
title: Partially Shared Query-Key for Lightweight Language Models
section: Model Efficiency \& Compression
abstract: Lightweight language models, such as TinyBERT 14.5M, have emerged as a critical
  area of research because of their implementation on resource-constrained hardware.
  These transformer models include significantly smaller parameter size, reduced memory
  and computational requirements. These features make such models highly suitable
  for deployment on small devices. We explore the concept of parameter sharing between
  the key and query weight matrices of a transformer model. The full query-key sharing
  which has already been proposed in the literature introduces a fully-quadratic attention
  matrix, oversimplifies directional dependencies and degrades pre-training loss.
  In contrast, partial parameter sharing balances complexity reduction and performance
  retention. Partial parameter sharing effectively addresses over-fitting while maintaining
  strong performance even with a high degree of shared parameters up to 95%. This
  provides a promising strategy for enhancing language models, specifically targeting
  small models.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: yang24a
month: 0
tex_title: Partially Shared Query-Key for Lightweight Language Models
firstpage: 286
lastpage: 291
page: 286-291
order: 286
cycles: false
bibtex_author: Yang, Kai and Partovi Nia, Vahid and Chen, Boxing and Asgharian, Masoud
author:
- given: Kai
  family: Yang
- given: Vahid
  family: Partovi Nia
- given: Boxing
  family: Chen
- given: Masoud
  family: Asgharian
date: 2024-12-10
address:
container-title: Proceedings of The 4th NeurIPS Efficient Natural Language and Speech
  Processing Workshop
volume: '262'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 12
  - 10
pdf: https://raw.githubusercontent.com/mlresearch/v262/main/assets/yang24a/yang24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
