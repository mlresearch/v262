---
title: 'QuAILoRA: Quantization-Aware Initialization for LoRA'
section: Training
abstract: QLoRA reduces the memory-cost of fine-tuning a large language model (LLM)
  with LoRA by quantizing the base LLM. However, quantization introduces quantization
  errors that negatively impact model performance after fine-tuning. In this paper
  we introduce QuAILoRA, a quantization-aware initialization for LoRA that mitigates
  this negative impact by decreasing quantization errors at initialization. Our method
  spends a small amount of computational overhead to compute this quantization-aware
  initialization, without increasing the memory-cost of fine-tuning. We evaluate our
  method on several causal language modeling and downstream evaluation tasks using
  several different model sizes and families. We observe that almost all LLMs fined-tuned
  with QuAILoRA achieve better validation perplexity. When evaluated on downstream
  tasks, we find that QuAILoRA yields improvements proportional to the negative effect
  of quantization error. On average, applying QuAILoRA to 4-bit QLoRA models yields
  75% of the validation perplexity decrease and 86% of the downstream task accuracy
  increase as doubling the quantization precision to 8-bit, without increasing GPU
  memory utilization during fine-tuning.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: g-lawton24a
month: 0
tex_title: "{QuAILoRA}: Quantization-Aware Initialization for {LoRA}"
firstpage: 22
lastpage: 33
page: 22-33
order: 22
cycles: false
bibtex_author: G Lawton, Neal and Padmakumar, Aishwarya and Gaspers, Judith and FitzGerald,
  Jack and Kumar, Anoop and Ver Steeg, Greg and Galstyan, Aram
author:
- given: Neal
  family: G Lawton
- given: Aishwarya
  family: Padmakumar
- given: Judith
  family: Gaspers
- given: Jack
  family: FitzGerald
- given: Anoop
  family: Kumar
- given: Greg
  family: Ver Steeg
- given: Aram
  family: Galstyan
date: 2024-12-10
address:
container-title: Proceedings of The 4th NeurIPS Efficient Natural Language and Speech
  Processing Workshop
volume: '262'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 12
  - 10
pdf: https://raw.githubusercontent.com/mlresearch/v262/main/assets/g-lawton24a/g-lawton24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
