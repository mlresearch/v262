---
title: Rephrasing natural text data with different languages and quality levels for
  Large Language Model pre-training
section: Benchmark \& Evaluation
abstract: Recently published work on rephrasing natural text data for pre-training
  LLMs has shown promising results when combining the original dataset with the synthetically
  rephrased data. We build upon previous work by replicating existing results on C4
  and extending them with our optimized rephrasing pipeline to the English, German,
  Italian, and Spanish Oscar subsets of CulturaX. Our pipeline leads to increased
  performance on standard evaluation benchmarks in both the mono- and multilingual
  setup. In addition, we provide a detailed study of our pipeline, investigating the
  choice of the base dataset and LLM for the rephrasing, as well as the relationship
  between the model size and the performance after pre-training. By exploring data
  with different perceived quality levels, we show that gains decrease with higher
  quality. Furthermore, we find the difference in performance between model families
  to be bigger than between different model sizes. This highlights the necessity for
  detailed tests before choosing an LLM to rephrase large amounts of data. Moreover,
  we investigate the effect of pre-training with synthetic data on supervised fine-tuning.
  Here, we find increasing but inconclusive results that highly depend on the used
  benchmark. These results (again) highlight the need for better benchmarking setups.
  In summary, we show that rephrasing multilingual and low-quality data is a very
  promising direction to extend LLM pre-training data.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: pieler24a
month: 0
tex_title: Rephrasing natural text data with different languages and quality levels
  for Large Language Model pre-training
firstpage: 491
lastpage: 511
page: 491-511
order: 491
cycles: false
bibtex_author: Pieler, Michael and Bellagente, Marco and Teufel, Hannah and Phung,
  Duy and Cooper, Nathan and Tow, Jonathan and Rocha, Paulo and Adithyan, Reshinth
  and Alyafeai, Zaid and Pinnaparaju, Nikhil and Zhuravinskyi, Maksym and Riquelme,
  Carlos
author:
- given: Michael
  family: Pieler
- given: Marco
  family: Bellagente
- given: Hannah
  family: Teufel
- given: Duy
  family: Phung
- given: Nathan
  family: Cooper
- given: Jonathan
  family: Tow
- given: Paulo
  family: Rocha
- given: Reshinth
  family: Adithyan
- given: Zaid
  family: Alyafeai
- given: Nikhil
  family: Pinnaparaju
- given: Maksym
  family: Zhuravinskyi
- given: Carlos
  family: Riquelme
date: 2024-12-10
address:
container-title: Proceedings of The 4th NeurIPS Efficient Natural Language and Speech
  Processing Workshop
volume: '262'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 12
  - 10
pdf: https://raw.githubusercontent.com/mlresearch/v262/main/assets/pieler24a/pieler24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
