---
title: 'Beyond Parameter Count: Implicit Bias in Soft Mixture of Experts'
section: Model Design \& Architecture
abstract: The traditional viewpoint on Sparse Mixture of Experts (MoE) models is that
  instead of training a single large expert, which is computationally expensive, we
  can train many small experts. The hope is that if the total parameter count of the
  small experts equals that of the singular large expert, then we retain the representation
  power of the large expert while gaining computational tractability and promoting
  expert specialization. The recently introduced Soft MoE replaces the Sparse MoE’s
  discrete routing mechanism with a differentiable gating function that smoothly mixes
  tokens. While this smooth gating function successfully mitigates the various training
  instabilities associated with Sparse MoE, it is unclear whether it induces implicit
  biases that affect Soft MoE’s representation power or potential for expert specialization.
  We prove that Soft MoE with a single arbitrarily powerful expert cannot represent
  simple convex functions. This justifies that Soft MoE’s success cannot be explained
  by the traditional viewpoint of many small experts collectively mimicking the representation
  power of a single large expert, and that multiple experts are actually necessary
  to achieve good representation power (even for a fixed total parameter count). Continuing
  along this line of investigation, we introduce a notion of expert specialization
  for Soft MoE, and while varying the number of experts yet fixing the total parameter
  count, we consider the following (computationally intractable) task. Given any input,
  how can we discover the expert subset that is specialized to predict this input’s
  label? We empirically show that when there are many small experts, the architecture
  is implicitly biased in a fashion that allows us to efficiently approximate the
  specialized expert subset. Our method can be easily implemented to potentially reduce
  computation during inference.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: chung24a
month: 0
tex_title: 'Beyond Parameter Count: Implicit Bias in Soft Mixture of Experts'
firstpage: 145
lastpage: 164
page: 145-164
order: 145
cycles: false
bibtex_author: Chung, Youngseog and Malik, Dhruv and Schneider, Jeff and Li, Yuanzhi
  and Singh, Aarti
author:
- given: Youngseog
  family: Chung
- given: Dhruv
  family: Malik
- given: Jeff
  family: Schneider
- given: Yuanzhi
  family: Li
- given: Aarti
  family: Singh
date: 2024-12-10
address:
container-title: Proceedings of The 4th NeurIPS Efficient Natural Language and Speech
  Processing Workshop
volume: '262'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 12
  - 10
pdf: https://raw.githubusercontent.com/mlresearch/v262/main/assets/chung24a/chung24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
