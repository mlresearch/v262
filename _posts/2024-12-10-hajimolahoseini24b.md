---
title: Accelerating the Low-Rank Decomposed Models
section: Model Efficiency \& Compression
abstract: 'Tensor decomposition is a mathematically supported technique for data compression.
  It consists of applying some kind of a Low Rank Decomposition technique on the tensors
  or matrices in order to reduce the redundancy of the data. However, it is not a
  popular technique for compressing the AI models duo to the high number of new layers
  added to the architecture after decomposition. Although the number of parameters
  could shrink significantly, it could result in the model be more than twice deeper
  which could add some latency to the training or inference. In this paper, we present
  a comprehensive study about how to modify low rank decomposition technique in AI
  models so that we could benefit from both high accuracy and low memory consumption
  as well as speeding up the training and inference. '
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: hajimolahoseini24b
month: 0
tex_title: Accelerating the Low-Rank Decomposed Models
firstpage: 222
lastpage: 231
page: 222-231
order: 222
cycles: false
bibtex_author: Hajimolahoseini, Habib and Ahmed, Walid and Wen, Shuangyue and Liu,
  Yang
author:
- given: Habib
  family: Hajimolahoseini
- given: Walid
  family: Ahmed
- given: Shuangyue
  family: Wen
- given: Yang
  family: Liu
date: 2024-12-10
address:
container-title: Proceedings of The 4th NeurIPS Efficient Natural Language and Speech
  Processing Workshop
volume: '262'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 12
  - 10
pdf: https://raw.githubusercontent.com/mlresearch/v262/main/assets/hajimolahoseini24b/hajimolahoseini24b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
