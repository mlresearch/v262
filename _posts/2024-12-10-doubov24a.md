---
title: 'Sparse Upcycling: Inference Inefficient Finetuning'
section: Model Design \& Architecture
abstract: Small, highly trained, open-source LLMs are widely used due to their inference
  efficiency, but further improving their quality remains a challenge. Sparse upcycling
  is a promising approach that transforms a pretrained dense model into a Mixture-of-Experts
  (MoE) architecture, increasing the modelâ€™s parameter count and potential quality.
  In this work, we compare the effectiveness of sparse upcycling against continued
  pretraining (CPT) across different model sizes, FLOP budgets, and pretraining durations.
  Our experiments show that sparse upcycling can achieve better quality, with improvements
  of over 20% relative to CPT in certain scenarios. However, this comes with a significant
  inference cost, leading to 40% slowdowns in high-demand inference settings for larger
  models. These results highlight the trade-off between model quality and inference
  efficiency, offering insights for practitioners seeking to balance performance with
  practical deployment costs.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: doubov24a
month: 0
tex_title: 'Sparse Upcycling: Inference Inefficient Finetuning'
firstpage: 194
lastpage: 205
page: 194-205
order: 194
cycles: false
bibtex_author: Doubov, Sasha and Sardana, Nikhil and Chiley, Vitaliy
author:
- given: Sasha
  family: Doubov
- given: Nikhil
  family: Sardana
- given: Vitaliy
  family: Chiley
date: 2024-12-10
address:
container-title: Proceedings of The 4th NeurIPS Efficient Natural Language and Speech
  Processing Workshop
volume: '262'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 12
  - 10
pdf: https://raw.githubusercontent.com/mlresearch/v262/main/assets/doubov24a/doubov24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
