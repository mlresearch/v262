---
title: 'KD-LoRA: A Hybrid Approach to Efficient Fine-Tuning with LoRA and Knowledge
  Distillation'
section: Training
abstract: 'Large language models (LLMs) have demonstrated remarkable performance across
  various downstream tasks. However, the high computational and memory requirements
  of LLMs are a major bottleneck. To address this, parameter-efficient fine-tuning
  (PEFT) methods such as low-rank adaptation (LoRA) have been proposed to reduce computational
  costs while ensuring minimal loss in performance. Additionally, knowledge distillation
  (KD) has been a popular choice for obtaining compact student models from teacher
  models. In this work, we present KD-LoRA, a novel fine-tuning method that combines
  LoRA with KD. Our results demonstrate that KD-LoRA achieves performance comparable
  to full fine-tuning (FFT) and LoRA while significantly reducing resource requirements.
  Specifically, KD-LoRA retains 98% of LoRAâ€™s performance on the GLUE benchmark, while
  being 40% more compact. Additionally, KD-LoRA reduces GPU memory usage by 30% compared
  to LoRA, while decreasing inference time by 30% compared to both FFT and LoRA. We
  evaluate KD-LoRA across three encoder-only models: BERT, RoBERTa, and DeBERTaV3.
  Code is available at https://github.com/rambodazimi/KD-LoRA.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: azimi24a
month: 0
tex_title: "{KD-LoRA}: A Hybrid Approach to Efficient Fine-Tuning with LoRA and Knowledge
  Distillation"
firstpage: 73
lastpage: 80
page: 73-80
order: 73
cycles: false
bibtex_author: Azimi, Rambod and Rishav, Rishav and Teichmann, Marek and Ebrahimi
  Kahou, Samira
author:
- given: Rambod
  family: Azimi
- given: Rishav
  family: Rishav
- given: Marek
  family: Teichmann
- given: Samira
  family: Ebrahimi Kahou
date: 2024-12-10
address:
container-title: Proceedings of The 4th NeurIPS Efficient Natural Language and Speech
  Processing Workshop
volume: '262'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 12
  - 10
pdf: https://raw.githubusercontent.com/mlresearch/v262/main/assets/azimi24a/azimi24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
