---
title: 'GEAR: An Efficient Error Reduction Framework for KV Cache Compression in LLM
  Inference'
section: Inference
abstract: 'Key-value (KV) caching has become the de-facto technique to accelerate
  generation speed for large language models (LLMs) inference. However, the growing
  cache demand with increasing sequence length has transformed LLM inference to be
  a memory bound problem, significantly constraining the system throughput. Existing
  methods rely on dropping unimportant tokens or quantizing entries group-wise. Such
  methods, however, often incur high approximation errors to represent the compressed
  matrices. The autoregressive decoding process further compounds the error of each
  step, resulting in critical deviation in model generation and deterioration of performance.
  To tackle this challenge, we propose GEAR, an efficient error reduction framework
  that augments a quantization scheme with two error reduction components and achieves
  near-lossless performance at high compression ratios. GEAR first applies quantization
  to majority of entries of similar magnitudes to ultra-low precision. It then employs
  a low-rank matrix to approximate the quantization error, and a sparse matrix to
  remedy individual errors from outlier entries.  By adeptly integrating three techniques,
  GEAR is able to fully exploit their synergistic potentials. Our experiments show
  that GEAR can maintain similar accuracy to that of FP16 cache with improvement up
  to 24.42% over the SOTA baselines at 2-bit compression. Additionally, compared to
  LLM inference with FP16 KV cache, GEAR can reduce peak-memory of up to $2.39\times$,
  bringing $2.1\times\sim 5.07\times$ throughput improvement. Our code will be publicly
  available. '
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: kang24a
month: 0
tex_title: "{GEAR}: An Efficient Error Reduction Framework for {KV} Cache Compression
  in {LLM} Inference"
firstpage: 305
lastpage: 321
page: 305-321
order: 305
cycles: false
bibtex_author: Kang, Hao and Zhang, Qingru and Kundu, Souvik and Jeong, Geonhwa and
  Liu, Zaoxing and Krishna, Tushar and Zhao, Tuo
author:
- given: Hao
  family: Kang
- given: Qingru
  family: Zhang
- given: Souvik
  family: Kundu
- given: Geonhwa
  family: Jeong
- given: Zaoxing
  family: Liu
- given: Tushar
  family: Krishna
- given: Tuo
  family: Zhao
date: 2024-12-10
address:
container-title: Proceedings of The 4th NeurIPS Efficient Natural Language and Speech
  Processing Workshop
volume: '262'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 12
  - 10
pdf: https://raw.githubusercontent.com/mlresearch/v262/main/assets/kang24a/kang24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
