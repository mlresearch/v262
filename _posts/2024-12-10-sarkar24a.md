---
title: Revisiting SMoE Language Models by Evaluating Inefficiencies with Task Specific
  Expert Pruning
section: Model Design \& Architecture
abstract: 'Sparse Mixture of Expert (SMoE) models have emerged as a scalable alternative
  to dense models in language modeling. These models use conditionally activated feedforward
  subnetworks in transformer blocks, allowing for a separation between total model
  parameters and per-example computation. However, large token-routed SMoE models
  face a significant challenge: during inference, the entire model must be used for
  a sequence or a batch, resulting in high latencies in a distributed setting that
  offsets the advantages of per-token sparse activation. Our research explores task-specific
  model pruning to inform decisions about designing SMoE architectures, mainly modulating
  the choice of expert counts in pretraining. We investigate whether such pruned models
  offer advantages over smaller SMoE models trained from scratch, when evaluating
  and comparing them individually on tasks. To that end, we introduce an adaptive
  task-aware pruning technique {\tt UNCURL} to reduce the number of experts per MoE
  layer in an offline manner post-training. Our findings reveal a threshold pruning
  factor for the reduction that depends on the number of experts used in pretraining,
  above which, the reduction starts to degrade model performance. These insights contribute
  to our understanding of model design choices when pretraining with SMoE architectures,
  particularly useful when considering task-specific inference optimization for later
  stages.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: sarkar24a
month: 0
tex_title: Revisiting {SMoE} Language Models by Evaluating Inefficiencies with Task
  Specific Expert Pruning
firstpage: 165
lastpage: 181
page: 165-181
order: 165
cycles: false
bibtex_author: Sarkar, Soumajyoti and Lausen, Leonard and Cevher, Volkan and Brox,
  Thomas and Zha, Sheng and Karypis, George
author:
- given: Soumajyoti
  family: Sarkar
- given: Leonard
  family: Lausen
- given: Volkan
  family: Cevher
- given: Thomas
  family: Brox
- given: Sheng
  family: Zha
- given: George
  family: Karypis
date: 2024-12-10
address:
container-title: Proceedings of The 4th NeurIPS Efficient Natural Language and Speech
  Processing Workshop
volume: '262'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 12
  - 10
pdf: https://raw.githubusercontent.com/mlresearch/v262/main/assets/sarkar24a/sarkar24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
