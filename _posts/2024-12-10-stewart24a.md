---
title: 'The N-Grammys: Accelerating Autoregressive Inference with Learning-Free Batched
  Speculation'
section: Inference
abstract: Speculative decoding aims to speed up autoregressive generation of a language
  model by verifying in parallel the tokens generated by a smaller draft model. In
  this work, we explore the effectiveness of learning-free, negligible-cost draft
  strategies, namely $N$-grams obtained from the model weights and the context. While
  the predicted next token of the base model is rarely the top prediction of these
  simple strategies, we observe that it is often within their top-$k$ predictions
  for small $k$. Based on this, we show that combinations of simple strategies can
  achieve significant inference speedups over different tasks. The overall performance
  is comparable to more complex methods, yet does not require expensive preprocessing
  or modification of the base model, and allows for seamless ‘plug-and-play’ integration
  into pipelines.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: stewart24a
month: 0
tex_title: 'The {N-Grammys}: Accelerating Autoregressive Inference with Learning-Free
  Batched Speculation'
firstpage: 322
lastpage: 335
page: 322-335
order: 322
cycles: false
bibtex_author: Stewart, Lawrence and Trager, Matthew and Gonugondla, Sujan and Soatto,
  Stefano
author:
- given: Lawrence
  family: Stewart
- given: Matthew
  family: Trager
- given: Sujan
  family: Gonugondla
- given: Stefano
  family: Soatto
date: 2024-12-10
address:
container-title: Proceedings of The 4th NeurIPS Efficient Natural Language and Speech
  Processing Workshop
volume: '262'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 12
  - 10
pdf: https://raw.githubusercontent.com/mlresearch/v262/main/assets/stewart24a/stewart24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
