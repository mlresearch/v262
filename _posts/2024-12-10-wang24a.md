---
title: 'CSKV: Training-Efficient Channel Shrinking for KV Cache in Long-Context Scenarios'
section: Inference
abstract: 'Large Language Models (LLMs) have been widely adopted to process long-context
  tasks. However, the large memory overhead of the key-value (KV) cache poses significant
  challenges in long-context scenarios. Existing training-free KV cache compression
  methods typically focus on quantization and token pruning, which have compression
  limits, and excessive sparsity can lead to severe performance degradation. Other
  methods design new architectures with less KV overhead but require significant training
  overhead. To address the above two drawbacks, we further explore the redundancy
  in the channel dimension and apply an architecture-level design with minor training
  costs. Therefore, we introduce CSKV, a training-efficient Channel Shrinking technique
  for KV cache compression: (1) We first analyze the singular value distribution of
  the KV cache, revealing significant redundancy and compression potential along the
  channel dimension. Based on this observation, we propose using low-rank decomposition
  for key and value layers and storing the low-dimension features. (2) To preserve
  model performance, we introduce a bi-branch KV cache, including a window-based full-precision
  KV cache and a low-precision compressed KV cache. (3) To reduce the training costs,
  we minimize the layer-wise reconstruction loss for the compressed KV cache instead
  of retraining the entire LLMs. Extensive experiments show that CSKV can reduce the
  memory overhead of the KV cache by 80% while maintaining the modelâ€™s long-context
  capability. Moreover, we show that our method can be seamlessly combined with quantization
  to further reduce the memory overhead, achieving a compression ratio of up to 95%.
  Code is available at https://github.com/wln20/CSKV.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: wang24a
month: 0
tex_title: "{CSKV}: Training-Efficient Channel Shrinking for {KV} Cache in Long-Context
  Scenarios"
firstpage: 468
lastpage: 484
page: 468-484
order: 468
cycles: false
bibtex_author: Wang, Luning and Li, Shiyao and Ning, Xuefei and Yuan, Zhihang and
  Yan, Shengen and Dai, Guohao and Wang, Yu
author:
- given: Luning
  family: Wang
- given: Shiyao
  family: Li
- given: Xuefei
  family: Ning
- given: Zhihang
  family: Yuan
- given: Shengen
  family: Yan
- given: Guohao
  family: Dai
- given: Yu
  family: Wang
date: 2024-12-10
address:
container-title: Proceedings of The 4th NeurIPS Efficient Natural Language and Speech
  Processing Workshop
volume: '262'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 12
  - 10
pdf: https://raw.githubusercontent.com/mlresearch/v262/main/assets/wang24a/wang24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
