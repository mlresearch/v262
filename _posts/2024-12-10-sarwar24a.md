---
title: 'StructMoE: Structured Mixture of Experts Using Low Rank Experts'
section: Model Design \& Architecture
abstract: We introduce StructMoE, a method to scale MoE architectures by augmenting
  experts with dynamic capacity using structured matrices we call Low Rank Experts
  (LoRE). These LoREs are selected on a per-expert and per-token basis using a secondary
  router specific to every expert and are entangled with the main expert in the up-projection
  phase of the expert before the activation function. Empirically, we find this approach
  to outperform an MoE baseline in terms of loss on a held out validation set.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: sarwar24a
month: 0
tex_title: "{StructMoE}: Structured Mixture of Experts Using Low Rank Experts"
firstpage: 182
lastpage: 193
page: 182-193
order: 182
cycles: false
bibtex_author: Sarwar, Zain and Panda, Ashwinee and Th\'erien, Benjamin and Rawls,
  Stephen and Das, Anirban and Balasubramaniam, Kartik and Kapusuzoglu, Berkcan and
  Zhang, Shixiong and Sahu, Sambit and Naphade, Milind and Chakraborty, Supriyo
author:
- given: Zain
  family: Sarwar
- given: Ashwinee
  family: Panda
- given: Benjamin
  family: Th√©rien
- given: Stephen
  family: Rawls
- given: Anirban
  family: Das
- given: Kartik
  family: Balasubramaniam
- given: Berkcan
  family: Kapusuzoglu
- given: Shixiong
  family: Zhang
- given: Sambit
  family: Sahu
- given: Milind
  family: Naphade
- given: Supriyo
  family: Chakraborty
date: 2024-12-10
address:
container-title: Proceedings of The 4th NeurIPS Efficient Natural Language and Speech
  Processing Workshop
volume: '262'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 12
  - 10
pdf: https://raw.githubusercontent.com/mlresearch/v262/main/assets/sarwar24a/sarwar24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
