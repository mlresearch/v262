---
title: Efficient Alignment of Large Language Models via Data Sampling
section: Training
abstract: Despite the capabilities of Large Language Models (LLMs), the output is
  not always safe or desirable. Aligning the models to human values is a critical
  step for the safe adoption of these models. Aligning LLMs employ huge amounts of
  data, computation, and time. Moreover, curating data with human feedback is expensive
  and takes time. Recent research depicts the benefit of data engineering in the fine-tuning
  and pre-training paradigms to bring down such costs. However, alignment differs
  from the afore-mentioned paradigms and it is unclear if data efficient alignment
  is feasible. In this work, we first aim to understand how the performance of LLM
  alignment scales with data. We find out that LLM alignment performance follows an
  exponential plateau pattern which tapers off post a rapid initial increase. We identify
  data subsampling as a viable method to reduce resources required for alignment.
  Further, we propose a methodology for efficient alignment by identifying a small
  high quality subset thereby reducing the computation and time required by alignment.
  We evaluate the proposed methodology over multiple datasets and compare the results.
  We find that the model aligned using our proposed methodology outperforms other
  sampling methods and performs comparable to the model aligned with the full dataset
  while using a fraction of the resources.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: khera24a
month: 0
tex_title: Efficient Alignment of Large Language Models via Data Sampling
firstpage: 55
lastpage: 72
page: 55-72
order: 55
cycles: false
bibtex_author: Khera, Amrit and Ghosh, Rajat and Dutta, Debojyoti
author:
- given: Amrit
  family: Khera
- given: Rajat
  family: Ghosh
- given: Debojyoti
  family: Dutta
date: 2024-12-10
address:
container-title: Proceedings of The 4th NeurIPS Efficient Natural Language and Speech
  Processing Workshop
volume: '262'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 12
  - 10
pdf: https://raw.githubusercontent.com/mlresearch/v262/main/assets/khera24a/khera24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
