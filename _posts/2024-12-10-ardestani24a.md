---
title: Text Summarization With Graph Attention Networks
section: Applications
abstract: This study aimed to leverage graph information, particularly Rhetorical
  Structure Theory (RST) and Co-reference (Coref) graphs, to enhance the performance
  of our baseline summarization models. Specifically, we experimented with a Graph
  Attention Network architecture to incorporate graph information. However, this architecture
  did not enhance the performance. Subsequently, we used a simple Multi-layer Perceptron
  architecture, which improved the results in our proposed model on our primary dataset,
  CNN/DM. Additionally, we annotated XSum dataset with RST graph information, establishing
  a benchmark for future graph-based summarization models. This secondary dataset
  posed multiple challenges, revealing both the merits and limitations of our models.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: ardestani24a
month: 0
tex_title: Text Summarization With Graph Attention Networks
firstpage: 540
lastpage: 553
page: 540-553
order: 540
cycles: false
bibtex_author: Ardestani, Mohammadreza and Chali, Yllias
author:
- given: Mohammadreza
  family: Ardestani
- given: Yllias
  family: Chali
date: 2024-12-10
address:
container-title: Proceedings of The 4th NeurIPS Efficient Natural Language and Speech
  Processing Workshop
volume: '262'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 12
  - 10
pdf: https://raw.githubusercontent.com/mlresearch/v262/main/assets/ardestani24a/ardestani24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
