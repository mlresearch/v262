---
title: 'SuperPos-Prompt: Enhancing Soft Prompt Tuning of Language Models with Superposition
  of Multi Token Embeddings'
section: Training
abstract: 'Soft prompt tuning techniques have recently gained traction as an effective
  strategy for the parameter-efficient tuning of pre-trained language models, particularly
  minimizing the required adjustment of model parameters. Despite their growing use,
  achieving optimal tuning with soft prompts, especially with smaller datasets, remains
  a substantial challenge. This study makes two contributions in this domain: (i)
  we introduce SuperPos-Prompt, a new reparameterization technique employing the superposition
  of multiple pre-trained vocabulary embeddings to improve the learning of soft prompts.
  Our experiments across several GLUE and SuperGLUE benchmarks consistently highlight
  SuperPos-Promptâ€™s superiority over Residual Prompt tuning, exhibiting an average
  score increase of +6.4 in T5-Small and +5.0 in T5-Base along with a faster convergence.
  Remarkably, SuperPos-Prompt occasionally outperforms even full fine-tuning methods.
  (ii) Additionally, we demonstrate enhanced performance and rapid convergence by
  omitting dropouts from the frozen network, yielding consistent improvements across
  various scenarios and tuning methods.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: ali-sadraei-javaheri24a
month: 0
tex_title: "{SuperPos-Prompt}: Enhancing Soft Prompt Tuning of Language Models with
  Superposition of Multi Token Embeddings"
firstpage: 34
lastpage: 46
page: 34-46
order: 34
cycles: false
bibtex_author: Ali Sadraei Javaheri, Mohammad and Asgari, Ehsaneddin and C. McHardy,
  Alice and R. Rabiee, Hamid
author:
- given: Mohammad
  family: Ali Sadraei Javaheri
- given: Ehsaneddin
  family: Asgari
- given: Alice
  family: C. McHardy
- given: Hamid
  family: R. Rabiee
date: 2024-12-10
address:
container-title: Proceedings of The 4th NeurIPS Efficient Natural Language and Speech
  Processing Workshop
volume: '262'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 12
  - 10
pdf: https://raw.githubusercontent.com/mlresearch/v262/main/assets/ali-sadraei-javaheri24a/ali-sadraei-javaheri24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
