---
title: 'Scaling Smart: Accelerating Large Language Model Pre-Training with Small Model
  Initialization'
section: Training
abstract: 'The pre-training phase of language models often begins with randomly initialized
  parameters. With the current trends in scaling models, training their large number
  of parameters can be extremely slow and costly. In contrast, small language models
  are less expensive to train, but they often cannot achieve the accuracy of large
  models. In this paper, we explore an intriguing idea to connect these two different
  regimes: Can we develop a method to initialize large language models using smaller
  pre-trained models? Will such initialization bring any benefits in terms of training
  time and final accuracy? In this paper, we introduce HyperCloning, a method that
  can expand the parameters of a pre-trained language model to those of a larger model
  with increased hidden dimensions. Our method ensures that the larger model retains
  the functionality of the smaller model. As a result, the larger model already inherits
  the predictive power and accuracy of the smaller model before the training starts.
  We demonstrate that training such an initialized model results in significant savings
  in terms of GPU hours required for pre-training large language models. Implementation
  of HyperCloning is available at https://github.com/apple/ml-hypercloning/tree/main.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: samragh24a
month: 0
tex_title: 'Scaling Smart: Accelerating Large Language Model Pre-Training with Small
  Model Initialization'
firstpage: 1
lastpage: 13
page: 1-13
order: 1
cycles: false
bibtex_author: Samragh, Mohammad and Mirzadeh, Seyed Iman and Alizadeh-Vahid, Keivan
  and Faghri, Fartash and Cho, Minsik and Nabi, Moin and Naik, Devang and Farajtabar,
  Mehrdad
author:
- given: Mohammad
  family: Samragh
- given: Seyed Iman
  family: Mirzadeh
- given: Keivan
  family: Alizadeh-Vahid
- given: Fartash
  family: Faghri
- given: Minsik
  family: Cho
- given: Moin
  family: Nabi
- given: Devang
  family: Naik
- given: Mehrdad
  family: Farajtabar
date: 2024-12-10
address:
container-title: Proceedings of The 4th NeurIPS Efficient Natural Language and Speech
  Processing Workshop
volume: '262'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 12
  - 10
pdf: https://raw.githubusercontent.com/mlresearch/v262/main/assets/samragh24a/samragh24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
