---
title: Efficiently Dispatching Flash Attention For Partially Filled Attention Masks
section: Inference
abstract: 'Transformers are widely used across various applications, many of which
  yield sparse or partially filled attention matrices. Examples include attention
  masks designed to reduce the quadratic complexity of attention, sequence packing
  techniques, and recent innovations like tree masking for fast validation in MEDUSA.
  Despite the inherent sparsity in these matrices, the state-of-the-art algorithm
  Flash Attention still processes them with quadratic complexity as though they were
  dense. In this paper, we introduce \textbf{Binary Block Masking}, a highly efficient
  modification that enhances Flash Attention by making it mask-aware. We further propose
  two optimizations: one tailored for masks with contiguous non-zero patterns and
  another for extremely sparse masks. Our experiments on attention masks derived from
  real-world scenarios demonstrate up to a 9x runtime improvement. The implementation
  will be publicly released to foster further research and application.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: sharma24a
month: 0
tex_title: Efficiently Dispatching Flash Attention For Partially Filled Attention
  Masks
firstpage: 423
lastpage: 442
page: 423-442
order: 423
cycles: false
bibtex_author: Sharma, Agniv and A. Geiping, Jonas
author:
- given: Agniv
  family: Sharma
- given: Jonas
  family: A. Geiping
date: 2024-12-10
address:
container-title: Proceedings of The 4th NeurIPS Efficient Natural Language and Speech
  Processing Workshop
volume: '262'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 12
  - 10
pdf: https://raw.githubusercontent.com/mlresearch/v262/main/assets/sharma24a/sharma24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
