---
title: Post-Training Statistical Calibration for Higher Activation Sparsity
section: Model Efficiency \& Compression
abstract: We present Statistical Calibrated Activation Pruning (SCAP), a post-training
  activation pruning framework that (1) generalizes sparsification by input activations
  of Fully-Connected layers for generic and flexible application across Transformers,
  and (2) features a simple Mode-Centering technique to pre-calibrate activation distributions
  for maximizing post-training sparsity. Our results demonstrate robust Pareto efficiency
  compared to prior methods, translating to a 1.5Ã— additional LLM decoding speedup
  against CATS[12] at iso model quality. SCAP effectiveness is empirically verified
  across a wide range of models, including recent Transformer Decoders, MoE, Mamba2,
  Encoding Transformer, and pre-quantized models, highlighting its practicality and
  scalability. The code is available at https://github.com/IntelLabs/SCAP.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: seng-chua24a
month: 0
tex_title: Post-Training Statistical Calibration for Higher Activation Sparsity
firstpage: 206
lastpage: 221
page: 206-221
order: 206
cycles: false
bibtex_author: Seng Chua, Vui and Pan, Yujie and Jain, Nilesh and Seng Chua, Vui
author:
- given: Vui
  family: Seng Chua
- given: Yujie
  family: Pan
- given: Nilesh
  family: Jain
- given: Vui
  family: Seng Chua
date: 2024-12-10
address:
container-title: Proceedings of The 4th NeurIPS Efficient Natural Language and Speech
  Processing Workshop
volume: '262'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 12
  - 10
pdf: https://raw.githubusercontent.com/mlresearch/v262/main/assets/seng-chua24a/seng-chua24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
