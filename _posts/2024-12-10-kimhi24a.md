---
title: Hysteresis Activation Function for Efficient Inference
section: Inference
abstract: The widely used ReLU is favored for its hardware efficiency yet suffers
  from issues such as the “dying ReLU” problem, where during training, neurons fail
  to activate and constantly remain at zero, as highlighted by Lu et al. \citep{lu2018collapse}.
  Traditional approaches to mitigate this issue often introduce more complex and less
  hardware-friendly activation functions. In this work, we propose a Hysteresis Rectified
  Linear Unit (HeLU), an efficient activation function designed to address the “dying
  ReLU” problem with minimal complexity. Unlike traditional activation functions with
  fixed thresholds for training and inference, HeLU employs a variable threshold that
  refines the backpropagation. This refined mechanism allows simpler activation functions
  to achieve competitive performance comparable to their more complex counterparts
  without introducing unnecessary complexity or requiring inductive biases. Empirical
  evaluations demonstrate that HeLU enhances model generalization across diverse datasets,
  offering a promising solution for efficient and effective inference suitable for
  a wide range of neural network architectures.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: kimhi24a
month: 0
tex_title: Hysteresis Activation Function for Efficient Inference
firstpage: 414
lastpage: 422
page: 414-422
order: 414
cycles: false
bibtex_author: Kimhi, Moshe and Kashani, Idan and Baskin, Chaim and Mendelson, Avi
author:
- given: Moshe
  family: Kimhi
- given: Idan
  family: Kashani
- given: Chaim
  family: Baskin
- given: Avi
  family: Mendelson
date: 2024-12-10
address:
container-title: Proceedings of The 4th NeurIPS Efficient Natural Language and Speech
  Processing Workshop
volume: '262'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 12
  - 10
pdf: https://raw.githubusercontent.com/mlresearch/v262/main/assets/kimhi24a/kimhi24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
