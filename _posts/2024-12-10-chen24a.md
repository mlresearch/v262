---
title: 'OnlySportsLM: Optimizing Sports-Domain Language Models with SOTA Performance
  under Billion Parameters'
section: Applications
abstract: 'This paper explores the potential of a small, domain-specific language
  model trained exclusively on sports-related data. We investigate whether extensive
  training data with specially designed small model structures can overcome model
  size constraints. The study introduces the OnlySports collection, comprising OnlySportsLM,
  OnlySports Dataset, and OnlySports Benchmark. Our approach involves: 1) creating
  a massive 600 billion tokens OnlySports Dataset from FineWeb, 2) optimizing the
  RWKV architecture for sports-related tasks, resulting in a 196M parameters model
  with 20-layer, 640-dimension structure, 3) training the OnlySportsLM on part of
  OnlySports Dataset, and 4) testing the resultant model on OnlySports Benchmark.
  OnlySportsLM achieves a 37.62%/34.08% accuracy improvement over previous 135M/360M
  state-of-the-art models and matches the performance of larger models such as SomlLM
  1.7B and Qwen 1.5B in the sports domain. Additionally, the OnlySports collection
  presents a comprehensive workflow for building high-quality, domain-specific language
  models, providing a replicable blueprint for efficient AI development across various
  specialized fields.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: chen24a
month: 0
tex_title: "{OnlySportsLM}: Optimizing Sports-Domain Language Models with {SOTA} Performance
  under Billion Parameters"
firstpage: 596
lastpage: 610
page: 596-610
order: 596
cycles: false
bibtex_author: Chen, Zexin and Li, Chengxi and Xie, Xiangyu and Dube, Parijat
author:
- given: Zexin
  family: Chen
- given: Chengxi
  family: Li
- given: Xiangyu
  family: Xie
- given: Parijat
  family: Dube
date: 2024-12-10
address:
container-title: Proceedings of The 4th NeurIPS Efficient Natural Language and Speech
  Processing Workshop
volume: '262'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 12
  - 10
pdf: https://raw.githubusercontent.com/mlresearch/v262/main/assets/chen24a/chen24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
