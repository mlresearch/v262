---
title: Post Training Quantization of Large Language Models with Microscaling Formats
section: Model Efficiency \& Compression
abstract: 'Large Language Models (LLMs) have distinguished themselves with outstanding
  performance in complex language modeling tasks, yet they come with significant computational
  and storage challenges. This paper explores the potential of quantization to mitigate
  these challenges. We systematically study the combined application of three well-known
  post-training techniques, SmoothQuant, AWQ, and GPTQ, and provide a comprehensive
  analysis of their interactions and implications for advancing LLM quantization.
  We enhance the versatility of these methods by enabling quantization to microscaling
  (MX) formats, extending the applicability of these PTQ algorithms beyond their original
  fixed-point format targets. We show that combining different PTQ methods enables
  us to quantize models to 4-bit weights and 8-bit activations using the MXINT format
  with negligible accuracy loss compared to the uncompressed baseline. '
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: sharify24a
month: 0
tex_title: Post Training Quantization of Large Language Models with Microscaling Formats
firstpage: 241
lastpage: 258
page: 241-258
order: 241
cycles: false
bibtex_author: Sharify, Sayeh and Saxena, Utkarsh and Xu, Zifei and Yazar, Wanzin
  and Soloveychik, Ilya and Wang, Xin
author:
- given: Sayeh
  family: Sharify
- given: Utkarsh
  family: Saxena
- given: Zifei
  family: Xu
- given: Wanzin
  family: Yazar
- given: Ilya
  family: Soloveychik
- given: Xin
  family: Wang
date: 2024-12-10
address:
container-title: Proceedings of The 4th NeurIPS Efficient Natural Language and Speech
  Processing Workshop
volume: '262'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 12
  - 10
pdf: https://raw.githubusercontent.com/mlresearch/v262/main/assets/sharify24a/sharify24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
